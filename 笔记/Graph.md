<!--
 * @Author: Suez_kip 287140262@qq.com
 * @Date: 2022-11-06 09:18:11
 * @LastEditTime: 2022-11-10 20:28:08
 * @LastEditors: Suez_kip
 * @Description: 
-->
# 漏洞挖掘中的图的使用

## AST

![图 1](../images/0003d529b121d7f72cee15acaf7a8de8b967f2aa23443c8c298b51ee9cd8ba31.png)  

抽象语法树特点：

- 无论是LL(1)文法，还是LR(1)等方法，都要求在语法分析构造出相同的语法树。即使是前端采用了不同的文法，都只需要改变前端代码，而不用连累到后端。
- 在构造语法树时，不依赖于语言的细节。

简单流程：

1. 词法分析，scanner。对每一个代码中的单词进行分析，并得出其性质；->数组
2. 语法分析，解析器；数组->语法树（AST会删除其中没有必要的token，如不完整的括号，这与CST（具体语法树）对应）；例babel

## 控制流与数据流

### CFG、CDG

cfg 控制流图->cdg 控制依赖图（由FDT前向支配树产生）：  
![图 3](../images/d0e50bfeb09e3ff1a99cb6a8574ee889845f28497135b40c98e1143fc5a36c0b.png)  

![图 7](../images/2deb881fdf0b51739e64f003625b0179a39ef034aa42cda5eddb17e1acade40e.png)  

![图 4](../images/34bdd133ec568fb6b7fc9291e0a94d593645ca442511f083cee853b9f296f517.png)  

所有从函数出口到S2的路径都一定会经过S5；比如S1 -> C1 和 C1 -> S1 互相抵消。不过这还剩一个问题，就是像 C1 -> S3 这种依赖是怎么产生的？

ACFG 源自于Genius项目见下文

### DFG、DDG

变量使用的依赖，eazy，pass

## 图嵌入

<https://zhuanlan.zhihu.com/p/62629465>

图分析的目的包括四种： ( a )节点分类，( b )链接预测，( c )聚类，( d )可视化
嵌入的思想是在向量空间中保持连接的节点彼此靠近。拉普拉斯特征映射（Laplacian Eigenmaps）和局部线性嵌入（Locally Linear Embedding ，LLE）
自2010年以来，关于图嵌入的研究已经转移到解决网络稀疏性的可伸缩图嵌入技术上。
保持一阶二阶网络邻近度

目前研究的难点和目标：  

- *属性选择*  节点的“良好”向量表示应保留图的结构和单个节点之间的连接。第一个挑战是选择嵌入应该保留的图形属性。考虑到图中所定义的距离度量和属性过多，这种选择可能很困难，性能可能取决于实际的应用场景。
- *可扩展性*  大多数真实网络都很大，包含大量节点和边。嵌入方法应具有可扩展性，能够处理大型图。定义一个可扩展的模型具有挑战性，尤其是当该模型旨在保持网络的全局属性时。
- *嵌入的维数*  实际嵌入时很难找到表示的最佳维数。例如，较高的维数可能会提高重建精度，但具有较高的时间和空间复杂性。较低的维度虽然时间、空间复杂度低，但无疑会损失很多图中原有的信息。

大体上可以将这些嵌入方法分为三大类：

- 基于因子分解的方法
- 基于随机游走的方法(源于word2vec)
    首先选择某一特定点为起始点，做随机游走得到点的序列，然后将这个得到的序列视为句子，用word2vec来学习，得到该点的表示向量。
- 基于深度学习的方法
- LINE
  PAPER SOURCE：ang J , Qu M , Wang M , et al. LINE: Large-scale information network embedding[J]. 24th International Conference on World Wide Web, WWW 2015, 2015.

## 图神经网络

***Scalable Graph-based Bug Search for Firmware Images***  
[论文链接](../AI漏洞挖掘/Graph/Scalable%20Graph-based%20Bug%20Search%20for%20Firmware%20Images.pdf)  

固件漏洞挖掘

参考计算机视觉中的技术，基于CFG的高阶向量的表示方法 Genius
  
计算机视觉图形检索的主要步骤：  

1. 原始特征提取
2. 码本生成
3. 特征编码
4. 在线搜索
离线索引（包括前三步）和在线搜索

### 文章提出的ACFG
  
![图 1](../images/cdebf2f7de4f85f6b87f956bf48ff41154e83a1a42b966c8982896d20f96f139.png)  
![图 4](../images/968ed2073433c3a3a94664f60c99289b955b3312263923cc8face66b466b9155.png)  
![图 2](../images/d73fe08ae22f4b3d70cb991e7fdf4f9b8892e771db860534866cbe71668763ba.png)  

其他方法MCS最大公共子图，效率有限

### 码本生成

从原始特征中学习一组分类：C={c1，c2，…，ck}，其中ci是第i个码字或“质心”，共分为两步：

- 相似性度量计算：
  二部图法量化相似性，并用结构特征添加来防止二部图无图结构特征造成的误差积累导致不精确  
  将两张ACFG图组合为一张二部图，每个匹配都与成本相关联。两个图的最小代价是映射上所有边代价的总和。二部图匹配可以有效地遍历所有映射，并以最小的代价选择从G1到G2的节点上的一对一映射。边缘成本由该边缘上两个基本块之间的距离计算。  
  ![图 6](../images/8e1d5d4507ea48ab477a94fb3eb35abb4b2fc3d0bd324bb44f64208e8f1c7d88.png)  
  如果特征是集合，我们使用Jaccard来计算集合差。  
2
  - 利用空图计算的归一化；  
    两个图的匹配成本大于一，并且与比较的ACFG的大小正相关。因此，我们将成本归一化以计算相似性得分。对于成本归一化，我们为每个比较的ACFG创建一个空ACFGΦ。空图中的每个节点都有一个空特征向量，并且空图的大小被设置为对应的比较图的大小。通过与这个空的ACFG进行比较，我们可以获得被比较图可以得到的最大匹配成本;  
    $\kappa(g_1,g_2)=1-\frac{cost(g_1,g_2)}{max(cost(g_1,\phi),cost(\phi,g_2))}$  

  学习目标是找到能够最大化不同ACFG的距离同时最小化等效ACFG距离的权重参数(人话：不同类差距最大化，近似类差距最小化)  

  雅卡尔指数（英语：Jaccard index），又称为交并比、雅卡尔相似系数，比较样本集的相似性与多样性的统计量，定义为交集与并集的比值；  
  雅卡尔距离（Jaccard distance）用于量度样本集之间的不相似度，其定义为1减去雅卡尔系数；
  有人将雅卡尔距离定义两集合对称差$A\Delta B=|A\cup B|-|A\cap B|$的大小与并集大小之间的比例

- 聚类

使用聚类算法：频谱聚类算法
[谱聚类的相关知识](https://www.cnblogs.com/pinard/p/6221564.html)

#### K-means聚类算法介绍

[K-means聚类算法](https://www.cnblogs.com/pinard/p/6164214.html)  

![图 10](../images/9a750b0e6c05358cd103a6e1bd3e40c978bbbdb0062e176f63715c04059d8e44.png)  

简单介绍：  

``` 上图a表达了初始的数据集，假设k=2。
  在图b中，我们随机选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心，
  然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，
  经过计算样本和红色质心和蓝色质心的距离，我们得到了所有样本点的第一轮迭代后的类别。
  此时我们对我们当前标记为红色和蓝色的点分别求其新的质心，如图4所示，
  新的红色质心和蓝色质心的位置已经发生了变动。
  图e和图f重复了我们在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。
  最终我们得到的两个类别如图f。
```

- ***传统K-means算法流程***  
  1. 经验确定k值；
  2. 随机选择k个样本作为初始质心；
  3. 计算其他点到已选质心的欧式距离，将最小值加入对应的聚类中；
  4. 重新以平均值计算该类的质心；
  5. 若质心不再变化则结束，否则继续执行第三步；
- ***K-means++算法流程***  
    优化质心选择算法：  
  1. 随机选择一个点作为起始质心；
  2. 计算为选择点到已选择点的距离（取各个点中的最小值），取其最大值作为下一个质心；
  3. 重复2直至完成k个点的质心选择；
- ***距离计算优化的elkan K-means算法***  
    执行两种规律，利用质心间的距离减少点到质心的计算距离：  
  1. 是对于一个样本点x和两个质心μj1,μj2。如果我们预先计算出了这两个质心之间的距离D(j1,j2)，则如果计算发现2D(x,j1)≤D(j1,j2),我们立即就可以知道D(x,j1)≤D(x,j2)。此时我们不需要再计算D(x,j2),也就是说省了一步距离计算。
  2. 规律是对于一个样本点x和两个质心μj1,μj2。我们可以得到D(x,j2)≥max{0,D(x,j1)−D(j1,j2)}。这个从三角形的性质也很容易得到。
- ***大样本优化 Mini Batch K-Means算法***  
  选取部分集中的样本首先进行传统K-means算法，batch size一般是通过无放回的随机采样得到的。  
  为了增加算法的准确性，我们一般会多跑几次Mini Batch K-Means算法，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。
